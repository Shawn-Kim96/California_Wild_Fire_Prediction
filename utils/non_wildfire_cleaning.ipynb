{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db05fc23-2301-4fef-a9a4-92666b014f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2153eb99-f1b0-4667-ad40-39ef248cb110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/cmpe257/wild_fire/utils\n",
      "/root/cmpe257/wild_fire\n"
     ]
    }
   ],
   "source": [
    "# This is to set the current working directory to the correct (for me at least)\n",
    "# Can skip if it does not apply to you\n",
    "print(os.getcwd())\n",
    "new_path = \"/root/cmpe257/wild_fire\"\n",
    "os.chdir(new_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bbd3f6a-3cc4-4b18-a624-b33163d5b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with non-wildfire records\n",
    "DATA_PATH = 'data/processed/non_wildfire/'\n",
    "DATASET_NAME_INPUT = 'processed_non_wildfire_dates.csv'\n",
    "df = pd.read_csv(DATA_PATH + DATASET_NAME_INPUT)\n",
    "\n",
    "# # Print the column names in the dataset\n",
    "# print(\"Columns in the dataset:\")\n",
    "# print(df.columns)\n",
    "\n",
    "# # Print the first 5 rows to inspect the data\n",
    "# print(\"\\nFirst 5 rows of the dataset:\")\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74d2e0f-500a-4adb-96e6-f96cd9d09adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data shape: (3100, 79)\n",
      "Columns in the dataset:\n",
      "Index(['date', 'county', 'latitude', 'longitude', 'DayAirTmpAvg01',\n",
      "       'DayPrecip01', 'DayRelHumAvg01', 'DaySoilTmpAvg01', 'DayWindSpdAvg01',\n",
      "       'DayAirTmpAvg02', 'DayPrecip02', 'DayRelHumAvg02', 'DaySoilTmpAvg02',\n",
      "       'DayWindSpdAvg02', 'DayAirTmpAvg03', 'DayPrecip03', 'DayRelHumAvg03',\n",
      "       'DaySoilTmpAvg03', 'DayWindSpdAvg03', 'DayAirTmpAvg04', 'DayPrecip04',\n",
      "       'DayRelHumAvg04', 'DaySoilTmpAvg04', 'DayWindSpdAvg04',\n",
      "       'DayAirTmpAvg05', 'DayPrecip05', 'DayRelHumAvg05', 'DaySoilTmpAvg05',\n",
      "       'DayWindSpdAvg05', 'DayAirTmpAvg06', 'DayPrecip06', 'DayRelHumAvg06',\n",
      "       'DaySoilTmpAvg06', 'DayWindSpdAvg06', 'DayAirTmpAvg07', 'DayPrecip07',\n",
      "       'DayRelHumAvg07', 'DaySoilTmpAvg07', 'DayWindSpdAvg07',\n",
      "       'DayAirTmpAvg08', 'DayPrecip08', 'DayRelHumAvg08', 'DaySoilTmpAvg08',\n",
      "       'DayWindSpdAvg08', 'DayAirTmpAvg09', 'DayPrecip09', 'DayRelHumAvg09',\n",
      "       'DaySoilTmpAvg09', 'DayWindSpdAvg09', 'DayAirTmpAvg10', 'DayPrecip10',\n",
      "       'DayRelHumAvg10', 'DaySoilTmpAvg10', 'DayWindSpdAvg10',\n",
      "       'DayAirTmpAvg11', 'DayPrecip11', 'DayRelHumAvg11', 'DaySoilTmpAvg11',\n",
      "       'DayWindSpdAvg11', 'DayAirTmpAvg12', 'DayPrecip12', 'DayRelHumAvg12',\n",
      "       'DaySoilTmpAvg12', 'DayWindSpdAvg12', 'DayAirTmpAvg13', 'DayPrecip13',\n",
      "       'DayRelHumAvg13', 'DaySoilTmpAvg13', 'DayWindSpdAvg13',\n",
      "       'DayAirTmpAvg14', 'DayPrecip14', 'DayRelHumAvg14', 'DaySoilTmpAvg14',\n",
      "       'DayWindSpdAvg14', 'DayAirTmpAvg', 'DayPrecip', 'DayRelHumAvg',\n",
      "       'DaySoilTmpAvg', 'DayWindSpdAvg'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Display initial info: shape and columns\n",
    "print(\"Initial data shape:\", df.shape)\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f3507dc-6683-4024-9c87-eee9fd712c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of columns: 79\n",
      "Threshold (minimum non-null values required per row): 64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the minimum number of non-null values required (80% of total columns)\n",
    "num_columns = df.shape[1]\n",
    "threshold = math.ceil(0.8 * num_columns)  # round up to ensure 80% coverage\n",
    "print(\"Total number of columns:\", num_columns)\n",
    "print(\"Threshold (minimum non-null values required per row):\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e045ad-70d0-44ea-ba73-c5b8cb0ed36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data shape: (2614, 79)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows that do not have at least 'threshold' non-null entries\n",
    "clean_df = df.dropna(thresh=threshold)\n",
    "print(\"Cleaned data shape:\", clean_df.shape) # (# rows, # columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "797030c2-dea2-4943-af72-597ee2688a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to cleaned_non_wildfire_dates.csv.\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned data to a new CSV file\n",
    "OUTPUT_DATANAME = 'cleaned_non_wildfire_dates.csv'\n",
    "clean_df.to_csv(DATA_PATH + OUTPUT_DATANAME, index=False)\n",
    "print(f\"Cleaned data saved to {OUTPUT_DATANAME}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
